{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f2fa9b",
   "metadata": {},
   "source": [
    "Q1. (Based on Step-by-Step Implementation of Ridge Regression using Gradient\n",
    "Descent Optimization)\n",
    "Generate a dataset with atleast seven highly correlated columns and a target variable.Implement Ridge Regression using Gradient Descent Optimization. Take different values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization parameter (10-15,10-10,10-5 ,10-3 ,0,1,10,20). Choose the best parameters for which ridge regression cost function is minimum and R2_score is maximum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a40dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights: [ 4.36362647  0.72743825 -0.15426604 -0.19621603  0.14231416  0.06998212\n",
      " -0.10744382]\n",
      "Min Cost: 0.0804300886414354\n",
      "Max R2 Score: 0.9113768066479556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Souvig\\AppData\\Local\\Temp\\ipykernel_20052\\1706107420.py:20: RuntimeWarning: overflow encountered in multiply\n",
      "  grad = (-2/m) * x.T.dot(y - y_pred) + 2 * lam * w\n",
      "C:\\Users\\Souvig\\AppData\\Local\\Temp\\ipykernel_20052\\1706107420.py:20: RuntimeWarning: overflow encountered in multiply\n",
      "  grad = (-2/m) * x.T.dot(y - y_pred) + 2 * lam * w\n",
      "C:\\Users\\Souvig\\AppData\\Local\\Temp\\ipykernel_20052\\1706107420.py:20: RuntimeWarning: overflow encountered in multiply\n",
      "  grad = (-2/m) * x.T.dot(y - y_pred) + 2 * lam * w\n",
      "C:\\Users\\Souvig\\AppData\\Local\\Temp\\ipykernel_20052\\1706107420.py:22: RuntimeWarning: overflow encountered in multiply\n",
      "  w -= lr * grad\n",
      "C:\\Users\\Souvig\\AppData\\Local\\Temp\\ipykernel_20052\\1706107420.py:22: RuntimeWarning: overflow encountered in multiply\n",
      "  w -= lr * grad\n",
      "C:\\Users\\Souvig\\AppData\\Local\\Temp\\ipykernel_20052\\1706107420.py:20: RuntimeWarning: overflow encountered in multiply\n",
      "  grad = (-2/m) * x.T.dot(y - y_pred) + 2 * lam * w\n",
      "C:\\Users\\Souvig\\AppData\\Local\\Temp\\ipykernel_20052\\1706107420.py:20: RuntimeWarning: overflow encountered in add\n",
      "  grad = (-2/m) * x.T.dot(y - y_pred) + 2 * lam * w\n",
      "C:\\Users\\Souvig\\AppData\\Local\\Temp\\ipykernel_20052\\1706107420.py:22: RuntimeWarning: overflow encountered in multiply\n",
      "  w -= lr * grad\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = np.random.rand(20, 7)\n",
    "y = 3 * x[:, 0] + 2 * x[:, 1] + np.random.randn(20) * 0.1 \n",
    "\n",
    "# Add bias column (for intercept)\n",
    "X = np.c_[np.ones(x.shape[0]), x]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x[:, 1:] = scaler.fit_transform(x[:, 1:])\n",
    "\n",
    "# Ridge Regression using Gradient Descent\n",
    "def ridge_regression(x, y, lr, lam, epochs=1000):\n",
    "    m, n = x.shape\n",
    "    w = np.zeros(n) #model weights\n",
    "    for _ in range(epochs):\n",
    "        y_pred = x.dot(w)\n",
    "        grad = (-2/m) * x.T.dot(y - y_pred) + 2 * lam * w\n",
    "        grad[0] -= 2 * lam * w[0]   # no regularization for bias term\n",
    "        w -= lr * grad\n",
    "        \n",
    "        if np.any(np.isnan(w)) or np.any(np.isinf(w)):\n",
    "            return np.zeros(n), np.inf\n",
    "    cost = np.mean((y - x.dot(w))**2) + lam * np.sum(w**2)\n",
    "    return w, cost\n",
    "\n",
    "lrs = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "lams = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
    "best = (None, float('inf'), -1)\n",
    "\n",
    "for lr in lrs:\n",
    "    for lam in lams:\n",
    "        w, cost = ridge_regression(x, y, lr, lam)\n",
    "        r2 = r2_score(y, x.dot(w))\n",
    "        if cost < best[1] and r2 > best[2]:\n",
    "            best = (w, cost, r2)\n",
    "\n",
    "print(\"Best weights:\", best[0])\n",
    "print(\"Min Cost:\", best[1])\n",
    "print(\"Max R2 Score:\", best[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b47903",
   "metadata": {},
   "source": [
    "Q2. Load the Hitters dataset from the following link\n",
    "https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing\n",
    "(a) Pre-process the data (null values, noise, categorical to numerical encoding)\n",
    "(b) Separate input and output features and perform scaling\n",
    "(c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use\n",
    "regularization parameter as 0.5748) regression function on the dataset.\n",
    "(d) Evaluate the performance of each trained model on test set. Which model\n",
    "performs the best and Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6993e",
   "metadata": {},
   "source": [
    "Unable to download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbee10",
   "metadata": {},
   "source": [
    "Q3. Cross Validation for Ridge and Lasso Regression\n",
    "Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV)\n",
    "function of Python. Implement both on Boston House Prediction Dataset (load_boston\n",
    "dataset from sklearn.datasets). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e295b4b",
   "metadata": {},
   "source": [
    "Using California Housing dataset as boston not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07c6f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge best alpha: 1.0\n",
      "Ridge R2: 0.5758157428925871\n",
      "Lasso best alpha: 0.1\n",
      "Lasso R2: 0.4813611325029077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = fetch_california_housing()\n",
    "x, y = data.data, data.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "ridge = RidgeCV(alphas=[0.1, 1, 10]).fit(x_train, y_train)\n",
    "lasso = LassoCV(alphas=[0.1, 1, 10]).fit(x_train, y_train)\n",
    "\n",
    "print(\"Ridge best alpha:\", ridge.alpha_)\n",
    "print(\"Ridge R2:\", r2_score(y_test, ridge.predict(x_test)))\n",
    "print(\"Lasso best alpha:\", lasso.alpha_)\n",
    "print(\"Lasso R2:\", r2_score(y_test, lasso.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cff72d",
   "metadata": {},
   "source": [
    "Q4. Multiclass Logistic Regression: Implement Multiclass Logistic Regression (step-by step)\n",
    "on Iris dataset using one vs. rest strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a49aa430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "x, y = load_iris(return_X_y=True)\n",
    "x = StandardScaler().fit_transform(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#logistic regression for one-vs-rest\n",
    "def train_lr(x, y, lr=0.1, epochs=1000):\n",
    "    m, n = x.shape\n",
    "    w = np.zeros(n)\n",
    "    for _ in range(epochs):\n",
    "        z = x.dot(w)\n",
    "        h = sigmoid(z)\n",
    "        grad = (1/m) * x.T.dot(h - y)\n",
    "        w -= lr * grad\n",
    "    return w\n",
    "\n",
    "weights = []\n",
    "for cls in np.unique(y):\n",
    "    y_bin = (y_train == cls).astype(int)\n",
    "    weights.append(train_lr(x_train, y_bin))\n",
    "\n",
    "def predict(x):\n",
    "    probs = [sigmoid(x.dot(w)) for w in weights]\n",
    "    return np.argmax(probs, axis=0)\n",
    "\n",
    "y_pred = predict(x_test)\n",
    "acc = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
